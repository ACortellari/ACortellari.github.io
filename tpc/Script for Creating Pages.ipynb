{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_page = \"[The Turkey-Palestine Source Collection](turkey_palestine_collection.md)\"\n",
    "separator = \" // \"\n",
    "\n",
    "page_heading_element = \"# \"\n",
    "page_heading_element_2 = \"## \"\n",
    "\n",
    "project_folder = 'tpc\\Sources Database'\n",
    "upper_category = 'firstlevel_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_view = pd.read_csv(rf'C:\\Users\\act1780\\Documents\\GitHub\\ACortellari.github.io\\{project_folder}\\articles_view.csv', delimiter='\\t')\n",
    "publications_view = pd.read_csv(fr'C:\\Users\\act1780\\Documents\\GitHub\\ACortellari.github.io\\{project_folder}\\publications_view.csv', delimiter='\\t')\n",
    "issues_table = pd.read_csv(fr'C:\\Users\\act1780\\Documents\\GitHub\\ACortellari.github.io\\{project_folder}\\issues_view.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Last Update Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_update = f\"\"\"\n",
    "```\n",
    "Last Update: {today}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of available views from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* articles_view * issues_view * publications_view'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = fr\"C:\\Users\\act1780\\Documents\\GitHub\\ACortellari.github.io\\{project_folder}/*\" \n",
    "files_list = glob.glob(folder_path)\n",
    "\n",
    "list_of_files = []\n",
    "for file_path in files_list:\n",
    "    file_path = file_path.split('.')[-2].split(\"\\\\\")[-1]\n",
    "    file_path = f\"* {file_path}\"\n",
    "    list_of_files.append(file_path)\n",
    "\n",
    "files = str(list_of_files).replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Main Publications Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_view['publication_title'] = publications_view.apply(lambda row: f\"[{row['publication_title']}](publications_{row['publication_title'].replace(' ', '_')}.md)\", axis=1)\n",
    "metadata = f\"In our dataset, we have {len(publications_view['publication_title'])} unique publication names. The earliest publication started in {min(publications_view['first_year'])}, while the latest publication ended in {max(publications_view['last_year'])}. These publications are spread across various locations including {', '.join(publications_view['publication_location'].unique())}.\"\n",
    "\n",
    "filename = rf\"{upper_category}publications.md\"\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(f\"{page_heading_element}{home_page}{separator}Publications\\n\\n\")\n",
    "    f.write(f\"{metadata}\\n\\n\")\n",
    "    f.write((publications_view).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create each Publications' Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_publications_articles = ['article_id', 'article_text', 'publication_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<csv.DictReader object at 0x000002505BDAC050>\n"
     ]
    }
   ],
   "source": [
    "csv_file = fr'C:\\Users\\act1780\\Documents\\GitHub\\ACortellari.github.io\\{project_folder}\\publications_view.csv'  # Change this to the path of your CSV file\n",
    "    \n",
    "with open(csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "        print(reader)\n",
    "        for row in reader:\n",
    "            publication_name = row['publication_title']\n",
    "            publication_type = row['publication_type']\n",
    "            start_year = row['first_year']\n",
    "            end_year = row['last_year']\n",
    "            total_issues = row['total_issues']\n",
    "            publication_location = row['publication_location']\n",
    "\n",
    "            metadata = f\"{publication_name} was a {publication_type} publication. It published {total_issues} issues in {publication_location} between {start_year} and {end_year}.\"\n",
    "            \n",
    "            filename = rf\"publications_{publication_name.replace(' ', '_')}.md\"\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(f\"{page_heading_element}[Publications]({upper_category}publications.md){separator}{publication_name}\\n\\n\")\n",
    "                f.write(f\"{metadata}\\n\\n\")\n",
    "                f.write(f\"{page_heading_element_2}Articles in Scope\\n\\n\")\n",
    "                f.write(f\"{articles_view[articles_view['publication_title'] == publication_name].drop(columns=columns_publications_articles).to_markdown(index=False)}\\n\\n\")\n",
    "                f.write(f\"{page_heading_element_2}Issues Summary\\n\\n\")\n",
    "                f.write(issues_table[issues_table['publication_title'] == publication_name].sort_values(by=['issue_year', 'issue_month']).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Article Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = rf\"{upper_category}articles.md\"\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(f\"{page_heading_element}{home_page}{separator}Articles\\n\\n\")\n",
    "    f.write((articles_view.drop(columns=['article_id', 'article_text'])).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create each articles' page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_with_text = articles_view[articles_view['article_text'].notna()]\n",
    "\n",
    "for index, row in articles_with_text.iterrows():\n",
    "    filename = rf\"articles_{row['article_title'].replace('?', '')}.md\"\n",
    "    metadata_table = articles_with_text.loc[articles_with_text['article_title'] == row['article_title']].drop(columns={'article_text'})\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"{page_heading_element}{row['article_title']}\\n\\n\")\n",
    "        f.write(f\"{metadata_table.to_markdown(index=False)}\\n\\n\")\n",
    "        #f.write(f\"**Author:** {row['author']}\\n\\n\")\n",
    "        #f.write(f\"**Published on:** [{row['publication_title']}]({row['publication_title']}.md)\\n\\n\")\n",
    "        #f.write(f\"**Issue:** {row['issue_number']}, {row['issue_date']}\\n\\n\")\n",
    "        #f.write(f\"**Pages:** {row['pages']}\\n\\n\")\n",
    "        f.write(f\"{row['article_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Collection main page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a main menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ARTICLES](firstlevel_articles.md) // [PUBLICATIONS](firstlevel_publications.md)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = r\"C:\\Users\\act1780\\Documents\\GitHub\\ACortellari.github.io\\tpc/first*.md\"  # Replace this with the path to your folder\n",
    "first_level_list = glob.glob(folder_path)\n",
    "\n",
    "menu = f'{separator}'.join([f\"[{levels.split(os.path.sep)[-1].split('_')[-1].split('.')[-2].upper()}]({levels.split(os.path.sep)[-1]})\" for levels in first_level_list])\n",
    "menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = rf\"turkey_palestine_collection.md\"\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(f\"{page_heading_element}{home_page}\\n\\n\")\n",
    "    f.write(f\"{page_heading_element}{menu}\")\n",
    "    f.write(f\"{last_update}\\n\\n\")\n",
    "    f.write(f\"This page is a humble list of sources, primarily collected from mainstream and leftist press, that can be useful to those interested in studying the history of the radical left in Turkey, specifically its linkages and intellectual connections with Palestinian organizations from the late 1960s to the early 1970s.\\n\\n\") \n",
    "    f.write(f\"The bulk of the corpus listed—and sometimes provided in textual form—here is the result of research efforts that occupied me in 2018-19 while working on the Master's Thesis as part of my MA in Middle Eastern Studies at Leiden University. I completed my thesis, titled <a href='https://studenttheses.universiteitleiden.nl/handle/1887/82728'>Bringing Palestine Home: A Transnational History of Turkey's Radical Left and Palestine (1967-1972)</a>, under the supervision of Dr. Alp A. Yenen.\\n\\n\") \n",
    "    f.write(f\"The rich corpus of primary sources accessible through the digitization efforts of Türkiye Sosyal Tarih Araştırma Vakfı (TÜSTAV) significantly facilitated my research. I am grateful for TÜSTAV's valuable contributions to my endeavor. If any user believes that a valuable source is missing from this list (and that is most likely the case), I welcome and encourage suggestions. Your input is invaluable in enhancing the comprehensiveness of this small collection.\\n\\n\")\n",
    "    f.write(f\"{page_heading_element_2}Sources in the Leftist Press\\n\\n\")\n",
    "    f.write(f\"The sources provided here span the years 1968 to 1971 representing just a fraction of the leftist weekly and monthly publications from that period. Although the initial corpus was more extensive, only the sources included in my bibliography are currently listed. Additional sources may be made available here in the future. Sources published in leftist publications will gradually be made available in the <a href='https://github.com/ACortellari/ACortellari.github.io/tree/main/Turkey%20Palestine%20Archive/Sources%20Database'>Sources Database</a> (as .csv files for ease in fruition).\\n\\n\") \n",
    "    f.write(f\"The database includes the following:\\n\\n\")\n",
    "    f.write(\"\\n\".join([file_path for file_path in list_of_files]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyPetProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
